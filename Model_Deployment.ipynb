{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying model to a new dataset (Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Model development part, we obtained the best model to be Gradient boosting with the following parameters:<br>\n",
    "learning_rate: 0.1<br>\n",
    "n_estimators = 700<br>\n",
    "max_features = 300<br>\n",
    "max_depth = 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration the long run-time while optimizing the model, the final model was not saved in the model development jupyter notebook and decided to be saved in this notebook. The process was carried out in the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final best Gradient Boosting model from model deveopment notebook\n",
    "gb6 = GradientBoostingRegressor(learning_rate = 0.1, n_estimators = 700,  max_features = 300, max_depth = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-10 13:46:34.891257 2021-09-10 13:59:20.262978\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset \"IMDB_only\" on which gb6 was trained\n",
    "IMDB_only = pd.read_csv('pre-processed_dataset/IMDB_only.csv', index_col = 0)\n",
    "\n",
    "# Split the IMDB_only dataset into X and y for model development \n",
    "y = IMDB_only['avg_vote']\n",
    "X = IMDB_only.drop(columns = 'avg_vote')\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "gb6.fit(X_train, y_train)\n",
    "end = datetime.datetime.now()\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5061722343580888"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted = gb6.predict(X_test)\n",
    "r2_score(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gb6\n",
    "best_model.version = '1.0'\n",
    "best_model.pandas_version = pd.__version__\n",
    "best_model.numpy_version = np.__version__\n",
    "best_model.X_columns = [col for col in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gb6, open('Best_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in = open('Best_model.pkl', 'rb')\n",
    "model = pickle.load(model_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('pre-processed_dataset/IMDB_Kaggle_common.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data2['avg_vote']\n",
    "X = data2.drop(columns = 'avg_vote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2585, 1088), (2585,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the target \n",
    "y_predict = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4745713164626669"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate r2_score\n",
    "r2_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final Gradient Boostimng model with r2_score : 0.50 was applied on the unseen movies to determine their ratings. The model performance was very close to train dataset with r2_score: 0.47. <br><br>\n",
    "\n",
    "**Final Note:** \n",
    "In this project, all the possible ML models (Simple linear Regression, Lasso Regression, Ridge Regression, Random Forest Regressor, Gardient Boosting Regressor) have been applied to obtain the best model performance with maximum possible hyperparameter tuning. However as the dataset is lacking some import features like quality of music, quality of picture, richness of language used, chorography etc, hence the most efficient model couldn't be built. <br><br>This is a project to show how a Data science project is developed and how a Data scientist needs to be very vigilant in every steps i.e acquring the data, cleaning the data, processing it with appropriate feature engineering and finally developing various models on it, choosing the best one, save it and appying it in unseen data.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
